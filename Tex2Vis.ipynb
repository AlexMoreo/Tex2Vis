{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {
    "collapsed": true
   },
   "level": 1,
   "source": [
    "Text2Vis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure you can inport all the required packages before proceeding any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import bz2\n",
    "import getopt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from io import open \n",
    "import tensorflow as tf\n",
    "from BatchReader import BatchReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caption files are extracted from MsCOCO using the provided API and compressed in bz2.\n",
    "The visual features are extracted from the fc6 layer of the hybrid CNN (if you don't want/can extrat your own features, we will be happy to share ours!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_captions_file = \"mscoco/train2014.sentences.txt.bz2\"\n",
    "val_captions_file = \"mscoco/val2014.sentences.txt.bz2\"\n",
    "tr_visual_embeddings_file = \"visualembeddings/COCO_hybridCNN_fc6.dat.txt\"\n",
    "val_visual_embeddings_file = \"visualembeddings/COCO_val2014_hybridCNN_fc6.dat.txt\"\n",
    "outpath = \"Text2VisPredictions\"\n",
    "modelpath = \"ModelParameters\"\n",
    "\n",
    "batch_size=100\n",
    "use_dropout=False\n",
    "keep_prob_val = 0.5 #only when use_dropout=True\n",
    "valid_size=5000 \n",
    "test_size =20000 \n",
    "prob_visual_loss=0.5\n",
    "hidden_size=1024\n",
    "output_dim=4096 \n",
    "\n",
    "l2factor=0.00000001\n",
    "\n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "if not os.path.exists(modelpath):\n",
    "    os.makedirs(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the BatchReader. This will instantiate one reader for the captions and other for the images.\n",
    "Whenever a new batch is requested, the BatchReader object samples 'batch_size' images and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating the batch-reader\nReading captions file <mscoco/train2014.sentences.txt.bz2>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] Read 82783 images-ids, 414113 captions 4339907 words, 5.00 captions/image, 10.48 words/caption\nReading captions file <mscoco/val2014.sentences.txt.bz2>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] Read 123287 images-ids, 616767 captions 6462616 words, 5.00 captions/image, 10.48 words/caption\nBuilding captions indexes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (min_word_occurrences>=5) has length 10358\n"
     ]
    }
   ],
   "source": [
    "#The training file (captions and visual embeddings) are used as training data, whereas the validation file (captions and visual embeddings) is split into validation and test\n",
    "\n",
    "print(\"Instantiating the batch-reader\")\n",
    "batch = BatchReader(tr_captions_file, val_captions_file, tr_visual_embeddings_file, val_visual_embeddings_file, batch_size=batch_size, random_caption_samples_from_image=1, valid_size=valid_size, test_size=test_size)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Loads in memory the validation and test batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the validation set [500 images]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the test set [200 images]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done!]\n"
     ]
    }
   ],
   "source": [
    "#get the validation and test sets\n",
    "valid_input_cap, valid_out_cap, valid_out_visual, inputCaptionOffset, _ , valid_img_labels = batch.getValidationSet() \n",
    "test_input_cap, _, _, test_caption_offsets, _, test_img_labels = batch.getTestSet()\n",
    "print(\"[Done!]\")"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Create the Graph in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built!\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# GRAPH\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Input/Output data.\n",
    "  #-------------------------------------------------------\n",
    "  caption_input  = tf.placeholder(tf.float32, shape=[batch.batch_size, batch.vocabulary_size])\n",
    "  caption_output = tf.placeholder(tf.float32, shape=[batch.batch_size, batch.vocabulary_size])\n",
    "  visual_embedding_output = tf.placeholder(tf.float32, shape=[batch.batch_size, output_dim])\n",
    "  \n",
    "  caption_validation_input = tf.constant(valid_input_cap, tf.float32, shape=[valid_size, batch.vocabulary_size])\n",
    "  caption_validation_output = tf.constant(valid_out_cap, tf.float32, shape=[valid_size, batch.vocabulary_size])\n",
    "  visual_validation_output = tf.constant([x.tolist() for x in valid_out_visual], tf.float32, shape=[valid_size, output_dim])\n",
    "  \n",
    "  caption_test_input = tf.constant(test_input_cap, tf.float32, shape=[test_size, batch.vocabulary_size])\n",
    "  \n",
    "  global_step = tf.placeholder(tf.float32) #training iteration    \n",
    "  keep_prob =  tf.placeholder(tf.float32) #dropout keep-probability\n",
    "  \n",
    "  # Model parameters\n",
    "  #-------------------------------------------------------\n",
    "  #caption-embedding\n",
    "  cap2vec_weights = tf.Variable(tf.truncated_normal([batch.vocabulary_size, hidden_size], stddev=1.0 / math.sqrt(hidden_size)), name=\"cap2vec_weights\") \n",
    "  cap2vec_biases = tf.Variable(tf.zeros([hidden_size]), name=\"cap2vec_biases\")\n",
    "  \n",
    "  #embedding-caption\n",
    "  vec2cap_weights = tf.Variable(tf.truncated_normal([hidden_size, batch.vocabulary_size], stddev=1.0 / math.sqrt(batch.vocabulary_size)))\n",
    "  vec2cap_biases = tf.Variable(tf.zeros([batch.vocabulary_size]))\n",
    "  \n",
    "  #embedding-visual\n",
    "  cemb2vemb_weights = tf.Variable(tf.truncated_normal([hidden_size, output_dim], stddev=1.0 / math.sqrt(output_dim)), name=\"cemb2vemb_weights\")\n",
    "  cemb2vemb_biases = tf.Variable(tf.zeros([output_dim]), name=\"cemb2vemb_biases\")\n",
    "  \n",
    "  # Add ops to save and restore all the variables.\n",
    "  saver = tf.train.Saver({\"cap2vec_weights\":cap2vec_weights, \"cap2vec_biases\":cap2vec_biases, \"cemb2vemb_weights\":cemb2vemb_weights, \"cemb2vemb_biases\":cemb2vemb_biases})\n",
    "  \n",
    "  # NNet \n",
    "  #-------------------------------------------------------\n",
    "  def drop(tensor):\n",
    "\treturn tf.nn.dropout(tensor, keep_prob) if use_dropout else tensor\n",
    "\n",
    "  def nnet(cap_input):\n",
    "\tcaption_embeddings = tf.nn.relu(tf.matmul(cap_input, cap2vec_weights) + cap2vec_biases)\n",
    "\tcaption_reconstruc = tf.nn.relu(tf.matmul(caption_embeddings, vec2cap_weights) + vec2cap_biases)\n",
    "\tvisual_prediction  = tf.nn.relu(tf.matmul(drop(caption_embeddings), cemb2vemb_weights) + cemb2vemb_biases) #+ (tf.matmul(caption_input, raw2vemb_weights) + raw2vemb_biases)\n",
    "\treturn caption_reconstruc, visual_prediction\n",
    "\t\n",
    "  caption_reconstruc, visual_prediction = nnet(caption_input) \n",
    "  \n",
    "  # Losses functions\n",
    "  #-------------------------------------------------------\n",
    "  l2loss = l2factor*(tf.nn.l2_loss(cemb2vemb_weights) + tf.nn.l2_loss(cemb2vemb_biases)) if l2factor>=0 else 0.0\n",
    "  visual_loss = tf.reduce_mean(tf.square(visual_prediction - visual_embedding_output)) + l2loss\n",
    "  caption_loss = tf.reduce_mean(tf.square((caption_output - caption_reconstruc)))\n",
    "  loss = visual_loss+caption_loss\n",
    "  \n",
    "  # Optimizers\n",
    "  #-------------------------------------------------------\n",
    "  visual_optimizer = tf.train.AdamOptimizer().minimize(visual_loss)\n",
    "  caption_optimizer = tf.train.AdamOptimizer().minimize(caption_loss)\n",
    "  \n",
    "  \n",
    "  # Validation graph\n",
    "  #-------------------------------------------------------\n",
    "  caption_validation_reconstruc, visual_validation_prediction = nnet(caption_validation_input) \n",
    "  validation_caption_loss = tf.reduce_mean(tf.square((caption_validation_input - caption_validation_reconstruc)))\n",
    "  validation_visual_loss = tf.reduce_mean(tf.square(visual_validation_prediction - visual_validation_output)) + l2loss\n",
    "   \n",
    "  # Test graph\n",
    "  #-------------------------------------------------------\n",
    "  caption_test_reconstruc, visual_test_prediction = nnet(caption_test_input) \n",
    "  \n",
    "  print(\"Graph built!\")"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Run the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ModelParameters/Text2Vis_H256_StochasticLoss0.5_COCO_hybridCNN_fc6.dat.txt.ckpt\nAverage loss at step 0: Loss=1.739591 Training [vl=1.739486 cl=0.000105 l2=0.000001 dr=False]\tValidation[vl=17.431971 (b=17.431971) cl=0.001038 (b=0.001038)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: Loss=17.626478 Training [vl=17.625590 cl=0.000888 l2=0.000010 dr=False]\tValidation[vl=17.336010 (b=17.336010) cl=0.000818 (b=0.000818)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 20: Loss=17.560132 Training [vl=17.559314 cl=0.000818 l2=0.000011 dr=False]\tValidation[vl=17.178839 (b=17.178839) cl=0.000769 (b=0.000769)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 30: Loss=16.772609 Training [vl=16.771820 cl=0.000790 l2=0.000013 dr=False]\tValidation[vl=16.583117 (b=16.583117) cl=0.000753 (b=0.000753)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 40: Loss=16.701923 Training [vl=16.701146 cl=0.000777 l2=0.000016 dr=False]\tValidation[vl=16.183655 (b=16.183655) cl=0.000731 (b=0.000731)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 50: Loss=15.844191 Training [vl=15.843412 cl=0.000778 l2=0.000022 dr=False]\tValidation[vl=15.509085 (b=15.509085) cl=0.000727 (b=0.000727)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 60: Loss=15.656494 Training [vl=15.655717 cl=0.000776 l2=0.000029 dr=False]\tValidation[vl=15.067328 (b=15.067328) cl=0.000720 (b=0.000720)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 70: Loss=15.130696 Training [vl=15.129933 cl=0.000762 l2=0.000037 dr=False]\tValidation[vl=14.774522 (b=14.774522) cl=0.000708 (b=0.000708)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 80: Loss=15.266150 Training [vl=15.265400 cl=0.000750 l2=0.000043 dr=False]\tValidation[vl=14.737125 (b=14.737125) cl=0.000684 (b=0.000684)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 90: Loss=14.393732 Training [vl=14.392989 cl=0.000743 l2=0.000048 dr=False]\tValidation[vl=14.601666 (b=14.601666) cl=0.000674 (b=0.000674)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ModelParameters/Text2Vis_H256_StochasticLoss0.5_COCO_hybridCNN_fc6.dat.txt.ckpt\nAverage loss at step 100: Loss=14.664913 Training [vl=14.664188 cl=0.000726 l2=0.000051 dr=False]\tValidation[vl=14.556973 (b=14.556973) cl=0.000667 (b=0.000667)]\n[Done!]\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# TRAINING\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "num_steps = 300001\n",
    "valid_step=100\n",
    "save_step=1000\n",
    "\n",
    "def predictionsFileName(outpath):\n",
    "\tglobal step\n",
    "\tlosstype = \"_StochasticLoss\"+str(prob_visual_loss) if prob_visual_loss < 1.0 else \"_VisualLoss\"\n",
    "\tvisualfeatsfile = tr_visual_embeddings_file[tr_visual_embeddings_file.rfind('/')+1:]\n",
    "\thidden_dim=\"_H\"+str(hidden_size)\n",
    "\tdrop_info= \"_Drop\"+str(keep_prob_val) if use_dropout else \"\"\n",
    "\treturn outpath+\"/Text2Vis\"+hidden_dim+losstype+drop_info+\"_\"+visualfeatsfile\n",
    "\t\n",
    "def saveResults(outpath):\n",
    "  filename = predictionsFileName(outpath)\n",
    "  with open(filename+\".val\", 'w') as val_file:\n",
    "    val_file.write(u'It\\tTrVL\\tTrCL\\tVaVL\\tVaCL\\n')\n",
    "    for (t,tvl,tcl,vvl,vcl) in valid_values:\n",
    "\t\tval_file.write(u'%d\\t%f\\t%f\\t%f\\t%f\\n' % (t,tvl,tcl,vvl,vcl))\n",
    "\n",
    "best_valid_cap_loss, best_valid_visual_loss, last_saved_visual_loss = 1000,1000,1000\n",
    "valid_values = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  loss_ave, l2_ave, tr_vl_ave, tr_cl_ave = 0, 0, 0 , 0\n",
    "  \n",
    "  for step in range(num_steps):\n",
    "   \n",
    "    input_captions, output_captions, output_visual,_,_,_ = batch.nextBatch()    \n",
    "    fd = {caption_input : input_captions, caption_output : output_captions, visual_embedding_output : output_visual, global_step : step, keep_prob : keep_prob_val}\n",
    "\t\n",
    "    to_optimize = visual_optimizer if random.random() < prob_visual_loss else caption_optimizer\n",
    "\t\n",
    "    _, l, l2, cl, vl  = session.run([to_optimize, loss, l2loss, caption_loss, visual_loss], feed_dict=fd)\n",
    "    \n",
    "    l2_ave += l2 \n",
    "    loss_ave += l\n",
    "    tr_vl_ave += vl\n",
    "    tr_cl_ave += cl\n",
    "    \n",
    "    if step % valid_step == 0:\n",
    "\t\tloss_ave /= valid_step\n",
    "\t\ttr_vl_ave /= valid_step\n",
    "\t\ttr_cl_ave /= valid_step\n",
    "\t\tva_vl_ave  = validation_visual_loss.eval({keep_prob : 1.0})\n",
    "\t\tva_cl_ave = validation_caption_loss.eval({keep_prob : 1.0})\n",
    "\t\tbest_valid_cap_loss = min (best_valid_cap_loss, va_cl_ave)\n",
    "\t\tbest_valid_visual_loss = min (best_valid_visual_loss, va_vl_ave)\n",
    "\t\tsaveResults(outpath)\n",
    "\t\tif step % save_step == 0 and best_valid_visual_loss < last_saved_visual_loss:\n",
    "\t\t\tlast_saved_visual_loss=best_valid_visual_loss\n",
    "\t\t\tmodel_file = predictionsFileName(modelpath)+\".ckpt\"\n",
    "\t\t\tsave_path = saver.save(session, model_file)\n",
    "\t\t\tprint(\"Model saved in file: %s\" % save_path)\n",
    "\t\tprint('Average loss at step %d: Loss=%f Training [vl=%f cl=%f l2=%f dr=%r]\\tValidation[vl=%f (b=%f) cl=%f (b=%f)]' % (step, loss_ave, tr_vl_ave, tr_cl_ave, l2_ave, use_dropout, va_vl_ave, best_valid_visual_loss, va_cl_ave, best_valid_cap_loss))\n",
    "\t\tvalid_values.append((step,tr_vl_ave,tr_cl_ave,va_vl_ave,va_cl_ave))\n",
    "\t\tloss_ave, l2_ave, tr_vl_ave, tr_cl_ave = 0, 0, 0, 0\n",
    "  \n",
    "  print(\"[Done!]\")"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Generate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters from ModelParameters/Text2Vis_H256_StochasticLoss0.5_COCO_hybridCNN_fc6.dat.txt.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining the predictions...\nSaving results in Text2VisPredictions/Text2Vis_H256_StochasticLoss0.5_COCO_hybridCNN_fc6.dat.txt.pred...\n...5% completed\n...10% completed\n...15% completed\n...20% completed\n...25% completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...30% completed\n...35% completed\n...40% completed\n...45% completed\n...50% completed\n...55% completed\n...60% completed\n...65% completed\n...70% completed\n...75% completed\n...80% completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...85% completed\n...90% completed\n...95% completed\n...100% completed\n[Done!]\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# TEST\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "filename = predictionsFileName(outpath)+\".pred\"\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # Restore variables from disk if needed!\n",
    "  print('Loading model parameters from %s' % model_file)\n",
    "  saver.restore(session, model_file)\n",
    "  \n",
    "  print('Obtaining the predictions...')\n",
    "  predictions = visual_test_prediction.eval({keep_prob : 1.0})\n",
    "  \n",
    "  print('Saving results in %s...' % filename)\n",
    "  with open(filename, 'w') as pred_file:\n",
    "    test_written=0\n",
    "    for i in xrange(len(predictions)):\n",
    "\t\tpred_str = (' '.join(('%.3f' % x) for x in predictions[i])).replace(\" 0.000\", \" 0\")\n",
    "\t\timg_id= test_img_labels[i]\n",
    "\t\tcap_id= test_caption_offsets[i]\n",
    "\t\tcap_txt=batch.captions_orig[img_id][cap_id]\n",
    "\t\tpred_file.write(\"%s\\t%d\\t%s\\t%s\\n\" % (img_id, cap_id, cap_txt, pred_str))\n",
    "\t\ttest_written += 1\n",
    "\t\tif test_written % (test_size/20) == 0:\n",
    "\t\t\tprint(\"...%d%% completed\" % int(test_written*100.0/test_size))\t\n",
    "  \n",
    "  print(\"[Done!]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}